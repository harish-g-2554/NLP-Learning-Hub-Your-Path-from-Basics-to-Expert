<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Module 4 — N-gram, POS, & NER</title>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700;800&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="{{ url_for('static', filename='styles.css') }}">
<head>
  <!-- ... your other head elements ... -->
  <style>
    .page-nav {
      padding: 12px 24px;
      background: rgba(17, 18, 28, 0.4);
      border-bottom: 1px solid #273041;
      margin-bottom: 24px;
    }
    .page-nav a {
      color: #cbd5ff;
      text-decoration: none;
      font-weight: 600;
      font-size: 0.9rem;
    }
    .page-nav a:hover {
      text-decoration: underline;
    }
    /* ... rest of your styles ... */
  </style>
</head>
<body>
  <div class="scene">
    <header class="page-nav">
      <a href="{{ url_for('modules') }}">&larr; Back to All Modules</a>
    </header>
  <style>
    .lesson-wrap{max-width:1200px;margin:0 auto;padding:8px 16px 96px}
    .modules-hero{max-width:960px;margin:0 auto 18px;text-align:center}
    .modules-title{font-size:clamp(1.8rem,4.8vw,3rem);margin:8px 0 6px}
    .modules-hero p{color:#9fb0d8}

    .term{margin:18px 0;padding:22px;border-radius:16px;background:linear-gradient(180deg,rgba(17,18,28,.75),rgba(17,18,28,.45));border:1px solid #273041;box-shadow:0 10px 30px #0003,0 0 0 1px #1f2333 inset}
    .term h2, .term h3 {margin:0 0 8px;color:#e6e8ff}
    .t-desc{color:#9aa6d1; line-height: 1.6;}
    .twocol{display:grid;grid-template-columns:1fr 1fr;gap:24px;margin-top:10px}
    @media (max-width:900px){.twocol{grid-template-columns:1fr}}

    .diagram{padding:12px;border:1px solid #2b3449;border-radius:12px;background:rgba(12,14,22,.45);margin:10px 0}
    .d-caption{font-size:.82rem;color:#91a0c7;margin-bottom:6px;font-weight:700}
    .txt{width:100%;border-radius:12px;border:1px solid #2b3449;background:#0f1220;color:#e8ebff;padding:10px}
    .mono{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}
    .infobox{background:linear-gradient(180deg,rgba(6,182,212,.08),rgba(124,58,237,.08));border:1px solid #334155;border-radius:12px;padding:10px 12px;color:#bdd3f4}
    .formula-box{ background: rgba(12,14,22,.6); padding: 15px; border-radius: 10px; text-align: center; margin: 10px 0; }
    .formula{ font-family: 'Times New Roman', serif; font-size: 1.2rem; color: #c7d2fe; }
    
    .table{width:100%;border-collapse:separate;border-spacing:0 6px}
    .table th,.table td{padding:10px 12px;vertical-align:top}
    .table thead th{color:#cbd5ff;text-align:left;border-bottom:1px solid #2b3449}
    .table tbody tr{background:rgba(12,14,22,.55);border:1px solid #2b3449}
  </style>
</head>
<body>
  <div class="scene">
    <header class="modules-hero">
      <h1 class="modules-title">Module 4: <span>N-gram, POS, & NER</span></h1>
      <p class="modules-sub">Learn to model language sequences, tag parts of speech, and recognize real-world entities.</p>
    </header>

    <main class="lesson-wrap">

      <!-- N-grams -->
      <section class="term" id="ngram">
        <h2>N-grams: Predicting the Next Word</h2>
        <div class="twocol">
            <div>
                <h3>What is an N-gram?</h3>
                <p class="t-desc">An <strong>N-gram</strong> is a sequence of N consecutive words from a text. They are a simple but powerful way to capture context.</p>
                <p class="t-desc">• <strong>Unigram (1-gram):</strong> "hello"<br>• <strong>Bigram (2-gram):</strong> "hello world"<br>• <strong>Trigram (3-gram):</strong> "hello world today"</p>
                <h3>Why & Where Are They Used?</h3>
                <p class="t-desc">N-grams are used to build statistical language models. Their main job is to calculate the probability of a word appearing after a sequence of other words. This is fundamental for:
                <br>• <strong>Autocomplete & Predictive Text:</strong> Suggesting the next word as you type.
                <br>• <strong>Spell Correction:</strong> Determining if a sequence of words is likely or a mistake.
                <br>• <strong>Machine Translation:</strong> Ensuring the output sentence is fluent and probable.
                </p>
            </div>
            <div>
                <h3>N-gram Probability Formula</h3>
                <p class="t-desc">The probability of a word given the previous words is calculated by counting how many times that sequence appeared and dividing it by the number of times the prefix appeared.</p>
                <div class="formula-box">
                    <p class="formula">P(w<sub>n</sub> | w<sub>1</sub>, ..., w<sub>n-1</sub>) ≈ Count(w<sub>1</sub>, ..., w<sub>n</sub>) / Count(w<sub>1</sub>, ..., w<sub>n-1</sub>)</p>
                </div>
            </div>
        </div>
        <div class="diagram">
            <div class="d-caption">N-gram Probability Calculator</div>
            <label for="ngramCorpus" class="lbl">1. Enter Corpus Text (to build the model)</label>
            <textarea id="ngramCorpus" class="txt" rows="4">the cat sat on the mat. the dog sat on the rug.</textarea>
            <div class="twocol" style="margin-top:1rem;">
                <div>
                    <label for="ngramN" class="lbl">2. Select N-gram Size</label>
                    <select id="ngramN" class="txt">
                        <option value="2">Bigram (N=2)</option>
                        <option value="3">Trigram (N=3)</option>
                    </select>
                </div>
                <div>
                    <label for="ngramPhrase" class="lbl">3. Enter a Phrase to Test</label>
                    <input id="ngramPhrase" class="txt mono" value="the cat sat">
                </div>
            </div>
            <div style="margin-top:10px">
                <button class="btn primary" id="btnNgram" type="button">Calculate Probability</button>
            </div>
        </div>
        <div id="ngram-results" style="display:none;" class="diagram">
            <div id="ngramProbabilityResult"></div>
        </div>
      </section>

      <!-- Perplexity -->
      <section class="term" id="perplexity">
        <h2>Perplexity: How Good is Our N-gram Model?</h2>
        <p class="t-desc">
          <strong>Perplexity</strong> is a measurement of how well a language model predicts a sample. It tells us how "surprised" our N-gram model is by a new sentence. <strong>A lower perplexity is better</strong>, meaning the model is less surprised and has a better grasp of the language's patterns.
        </p>
        <div class="formula-box">
            <p class="formula">Perplexity = (P(w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>N</sub>))<sup>-1/N</sup></p>
        </div>
        <div class="diagram">
            <div class="d-caption">Perplexity Calculator</div>
            <label for="perpCorpus" class="lbl">1. Enter Corpus Text (to build the model)</label>
            <textarea id="perpCorpus" class="txt" rows="4">i want english food. sam and i like green vegetables. i like food and english movies.</textarea>
            <label for="perpSentence" class="lbl" style="margin-top:1rem;">2. Enter a Sentence to Evaluate</label>
            <input id="perpSentence" class="txt mono" value="i like green food">
            <div style="margin-top:10px">
                <button class="btn primary" id="btnPerplexity" type="button">Calculate Perplexity</button>
            </div>
        </div>
        <div id="perplexity-results" style="display:none;" class="diagram">
            <div id="perplexityResultText"></div>
        </div>
      </section>

      <!-- HMMs -->
      <section class="term" id="hmm">
        <h2>The Engine Behind Classic Tagging: Hidden Markov Models (HMMs)</h2>
        <div class="twocol">
            <div>
                <h3>What is an HMM?</h3>
                <p class="t-desc">A <strong>Hidden Markov Model (HMM)</strong> is a statistical model used for sequence labeling. It assumes that the sequence was generated by a process with unobserved ("hidden") states.</p>
                <div class="infobox">
                  <strong>Analogy:</strong> Imagine you're in a windowless room and want to know the weather. You can't see the weather (the <strong>hidden state</strong>: rainy or sunny), but you can see if your friend who enters is carrying an umbrella (the <strong>observation</strong>). An HMM works the same way: it guesses the hidden POS tag based on the observed word.
                </div>
            </div>
            <div>
                <h3>Why & Where Is It Used?</h3>
                <p class="t-desc">HMMs are excellent for any problem where you need to label a sequence of observations. They were a cornerstone of NLP for years, used in:
                <br>• <strong>Part-of-Speech (POS) Tagging:</strong> The classic use case.
                <br>• <strong>Speech Recognition:</strong> Mapping audio signals to words.
                <br>• <strong>Bioinformatics:</strong> Finding genes in DNA sequences.
                </p>
            </div>
        </div>
        <h3>Key Formulas</h3>
        <div class="twocol">
            <div>
                <p class="t-desc"><strong>Transition Probability:</strong> The probability of moving from one hidden state to another.</p>
                <div class="formula-box">
                    <p class="formula">P(tag₂ | tag₁) = Count(tag₁, tag₂) / Count(tag₁)</p>
                </div>
            </div>
            <div>
                <p class="t-desc"><strong>Emission Probability:</strong> The probability of observing a word, given a hidden state.</p>
                <div class="formula-box">
                    <p class="formula">P(word | tag) = Count(word, tag) / Count(tag)</p>
                </div>
            </div>
        </div>
      </section>

      <!-- POS Tagging -->
      <section class="term" id="pos">
        <h2>Part-of-Speech (POS) Tagging</h2>
        <p class="t-desc"><strong>Part-of-Speech (POS) Tagging</strong> is the process of labeling each word in a sentence with its grammatical role, like noun, verb, adjective, etc. It helps computers understand the structure and meaning of a sentence.</p>
        <div class="diagram">
            <div class="d-caption">POS Tagger</div>
            <label for="posSentence" class="lbl">Enter a sentence</label>
            <input id="posSentence" class="txt mono" value="The quick brown fox jumps over the lazy dog.">
            <div style="margin-top:10px">
                <button class="btn primary" id="btnPos" type="button">Tag Sentence</button>
            </div>
        </div>
        <div id="pos-results" style="display:none;" class="diagram">
            <div id="posTable"></div>
        </div>
      </section>

      <!-- NER -->
      <section class="term" id="ner">
        <h2>Named Entity Recognition (NER)</h2>
        <p class="t-desc"><strong>Named Entity Recognition (NER)</strong> is a step beyond POS tagging. It scans text to locate and classify "named entities"—real-world objects—into predefined categories such as persons, organizations, locations, dates, etc.</p>
        <div class="diagram">
            <div class="d-caption">NER Tagger</div>
            <label for="nerText" class="lbl">Enter text</label>
            <textarea id="nerText" class="txt" rows="3">Tim Cook, CEO of Apple Inc., announced the new iPhone in Cupertino on Tuesday.</textarea>
            <div style="margin-top:10px">
                <button class="btn primary" id="btnNer" type="button">Find Entities</button>
            </div>
        </div>
        <div id="ner-results" style="display:none;" class="diagram">
            <div id="nerTable"></div>
        </div>
      </section>
      
      <!-- QUIZ -->
      <section class="term">
        <h2>Quick Quiz</h2>
        <details class="qa"><summary><span class="qnum">1.</span> What is a 3-gram (trigram) of the sentence "I love NLP"?</summary><div class="answer">The only trigram is ("I", "love", "NLP").</div></details>
        <details class="qa"><summary><span class="qnum">2.</span> In POS tagging, what does 'NNP' usually stand for?</summary><div class="answer">Proper Noun, Singular (e.g., "London", "Google").</div></details>
        <details class="qa"><summary><span class="qnum">3.</span> What is the main goal of Named Entity Recognition (NER)?</summary><div class="answer">To locate and classify named entities in text into pre-defined categories such as persons, organizations, locations, etc.</div></details>
        <details class="qa"><summary><span class="qnum">4.</span> What are the "hidden" states in an HMM for POS tagging?</summary><div class="answer">The Part-of-Speech tags themselves, since we can't observe them directly from the text.</div></details>
        <details class="qa"><summary><span class="qnum">5.</span> What is an emission probability in the context of HMMs for POS tagging?</summary><div class="answer">The probability of observing a specific word given a specific POS tag, like P("cat" | Noun).</div></details>
        <details class="qa"><summary><span class="qnum">6.</span> A low perplexity score for a language model means...?</summary><div class="answer">The model is good at predicting the test sentence. A lower score is better.</div></details>
        <details class="qa"><summary><span class="qnum">7.</span> What does the "B" in the IOB format for NER stand for?</summary><div class="answer">"Beginning", indicating the first token of a multi-token named entity.</div></details>
        <details class="qa"><summary><span class="qnum">8.</span> Why are N-grams useful for language modeling?</summary><div class="answer">They capture local word context, which helps in predicting the next word in a sequence.</div></details>
        <details class="qa"><summary><span class="qnum">9.</span> What is a transition probability in an HMM?</summary><div class="answer">The probability of moving from one hidden state to another, e.g., P(Verb | Adjective).</div></details>
        <details class="qa"><summary><span class="qnum">10.</span> If a word is not in the training vocabulary, what problem does this cause for a simple N-gram model?</summary><div class="answer">It will have a probability of zero, which can make perplexity infinite. This is known as the "Out-of-Vocabulary" (OOV) problem.</div></details>
      </section>

    </main>
  </div>

  <script>
    async function postJSON(url, payload) {
      const res = await fetch(url, {
        method: 'POST',
        headers: {'Content-Type':'application/json'},
        body: JSON.stringify(payload),
      });
      if (!res.ok) {
        const errData = await res.json().catch(() => ({ error: `HTTP ${res.status}` }));
        throw new Error(errData.error);
      }
      return res.json();
    }

    // ---- N-gram Lab ----
    document.getElementById('btnNgram').addEventListener('click', async () => {
        const corpus = document.getElementById('ngramCorpus').value;
        const n = document.getElementById('ngramN').value;
        const phrase = document.getElementById('ngramPhrase').value;
        if (!corpus || !phrase) {
            alert('Please provide both a corpus and a test phrase.');
            return;
        }
        try {
            const data = await postJSON('/api/ngram/probability', { corpus, n: parseInt(n), phrase });
            let resultHtml = `<h3>Calculation Results</h3><p class="t-desc">${data.explanation}</p>`;
            resultHtml += `<div class="formula-box"><p class="formula">Final Probability: ${data.probability.toExponential(4)}</p></div>`;
            document.getElementById('ngramProbabilityResult').innerHTML = resultHtml;
            document.getElementById('ngram-results').style.display = 'block';
        } catch (e) {
            alert(`Error: ${e.message}`);
        }
    });

    // ---- Perplexity Lab ----
    document.getElementById('btnPerplexity').addEventListener('click', async () => {
        const corpus = document.getElementById('perpCorpus').value;
        const sentence = document.getElementById('perpSentence').value;
        if (!corpus || !sentence) {
            alert('Please provide both a corpus and a sentence.');
            return;
        }
        try {
            const data = await postJSON('/api/perplexity', { corpus, sentence });
            let resultText;
            if (data.perplexity === 'Infinity') {
                resultText = 'Perplexity: ∞ (Infinity) - The sentence contains word sequences not found in the corpus.';
            } else {
                resultText = `Perplexity of the sentence: ${data.perplexity.toFixed(4)}`;
            }
            document.getElementById('perplexityResultText').innerText = resultText;
            document.getElementById('perplexity-results').style.display = 'block';
        } catch (e) {
            alert(`Error: ${e.message}`);
        }
    });

    // ---- POS Lab ----
    document.getElementById('btnPos').addEventListener('click', async () => {
        const sentence = document.getElementById('posSentence').value;
        if (!sentence) {
            alert('Please enter a sentence.');
            return;
        }
        try {
            const data = await postJSON('/api/pos-tag', { sentence });
            let tableHtml = '<table class="table"><thead><tr><th>Word</th><th>Tag</th><th>Description</th></tr></thead><tbody>';
            data.tags.forEach(item => {
                tableHtml += `<tr><td>${item.word}</td><td class="mono">${item.tag}</td><td>${item.description}</td></tr>`;
            });
            tableHtml += '</tbody></table>';
            document.getElementById('posTable').innerHTML = tableHtml;
            document.getElementById('pos-results').style.display = 'block';
        } catch (e) {
            alert(`Error: ${e.message}`);
        }
    });

    // ---- NER Lab ----
    document.getElementById('btnNer').addEventListener('click', async () => {
        const text = document.getElementById('nerText').value;
        if (!text) {
            alert('Please enter text.');
            return;
        }
        try {
            const data = await postJSON('/api/ner', { text });
            let tableHtml = '<table class="table"><thead><tr><th>Word</th><th>Tag</th><th>Description</th></tr></thead><tbody>';
            data.entities.forEach(item => {
                tableHtml += `<tr><td>${item.word}</td><td class="mono">${item.tag}</td><td>${item.description}</td></tr>`;
            });
            tableHtml += '</tbody></table>';
            document.getElementById('nerTable').innerHTML = tableHtml;
            document.getElementById('ner-results').style.display = 'block';
        } catch (e) {
            alert(`Error: ${e.message}`);
        }
    });
  </script>
</body>
</html>
